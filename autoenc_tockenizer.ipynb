{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#print np in non-scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# print torch data in non-scientific notation\n",
    "torch.set_printoptions(sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_feather('data/nq17-23_1min.feather')\n",
    "df.index = df.index - pd.Timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_size = 0.25\n",
    "df['volume'] = df['volume'].astype(float)\n",
    "df['body'] = (df['close'] - df['open']) / tick_size\n",
    "df['top_wick'] = ((df['high'] - df[['open', 'close']].max(axis=1)) / tick_size)\n",
    "df['bottom_wick'] = ((df[['open', 'close']].min(axis=1) - df['low']) / tick_size)\n",
    "#df['direction'] = np.sign(df['close'] - df['open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "candles = df[['volume','top_wick','body', 'bottom_wick']].values\n",
    "#standardize candles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "candles = scaler.fit_transform(candles)\n",
    "\n",
    "candles = torch.tensor(candles, dtype=torch.float32)\n",
    "\n",
    "split_idx = int(len(candles) * 0.8)\n",
    "train_ds = TensorDataset(candles[:split_idx])\n",
    "valid_ds = TensorDataset(candles[split_idx:])\n",
    "\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandleAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, latent_size),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, input_size),\n",
    "        )\n",
    "        self.weigths_init(self.encoder)\n",
    "        self.weigths_init(self.decoder)\n",
    "\n",
    "\n",
    "    def weigths_init(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 64\n",
    "latent_size = 2\n",
    "def init_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "    #device = torch.device('cpu')\n",
    "\n",
    "    print(f\"Device to be used: {device}\")\n",
    "    #Initialize model\n",
    "    torch.manual_seed(42)\n",
    "    model = CandleAutoencoder(input_size, hidden_size, latent_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    #print(model)\n",
    "    #print model device\n",
    "    next(model.parameters()).device\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used: mps\n"
     ]
    }
   ],
   "source": [
    "model,device = init_model()\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "best_vloss = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, batch 29601/29661, valid loss: 0.2666924558\n",
      "Best model so far, saving...\n",
      "\n",
      "Epoch 1/5, train loss: 0.000001, valid loss: 0.00000899\n",
      "Epoch 2/5, batch 29601/29661, valid loss: 0.2437081437\n",
      "Best model so far, saving...\n",
      "\n",
      "Epoch 2/5, train loss: 0.000001, valid loss: 0.00000821\n",
      "Epoch 3/5, batch 29601/29661, valid loss: 0.2288503397\n",
      "Best model so far, saving...\n",
      "\n",
      "Epoch 3/5, train loss: 0.000001, valid loss: 0.00000771\n",
      "Epoch 4/5, batch 29601/29661, valid loss: 0.2202829105\n",
      "Best model so far, saving...\n",
      "\n",
      "Epoch 4/5, train loss: 0.000001, valid loss: 0.00000742\n",
      "Epoch 5/5, batch 29601/29661, valid loss: 0.2136756995\n",
      "Best model so far, saving...\n",
      "\n",
      "Epoch 5/5, train loss: 0.000001, valid loss: 0.00000720\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(train_dl):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"\\rEpoch {epoch+1}/{num_epochs}, batch {batch_idx+1}/{len(train_dl)}, train loss: {train_loss/(batch_idx+1):.8f}\", end=\"\\r\", flush=True)\n",
    "    train_loss /= (batch_idx + 1)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data,) in enumerate(valid_dl):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, data)\n",
    "            valid_loss += loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"\\rEpoch {epoch+1}/{num_epochs}, batch {batch_idx+1}/{len(valid_dl)}, valid loss: {valid_loss/(batch_idx+1):.8f}\", end=\"\\r\", flush=True)\n",
    "    valid_loss /= (batch_idx + 1)\n",
    "\n",
    "    if valid_loss/len(valid_dl) < best_vloss:\n",
    "        best_vloss = valid_loss/len(valid_dl)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        wd = optimizer.param_groups[0]['weight_decay']\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss/len(train_dl),\n",
    "            'valid_loss': valid_loss/len(valid_dl),\n",
    "            'learning_rate': lr,\n",
    "            'weight_decay': wd,\n",
    "            'hidden_size': hidden_size,\n",
    "            'latent_size': latent_size,\n",
    "        }, 'models/candle_autoencoder.pt')\n",
    "        print(f\"\\nBest model so far, saving...\\n\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, train loss: {train_loss/len(train_dl):.6f}, valid loss: {valid_loss/len(valid_dl):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 148303/148303\r"
     ]
    }
   ],
   "source": [
    "#load best model\n",
    "\n",
    "checkpoint = torch.load('models/candle_autoencoder.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "output = torch.empty(candles.shape[0], latent_size)\n",
    "#run model encoder on validation set\n",
    "model.eval()\n",
    "complete_dataset = TensorDataset(candles)\n",
    "complete_dataloader = DataLoader(complete_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (data,) in enumerate(complete_dataloader):\n",
    "        data = data.to(device)\n",
    "        out = (model.encoder(data).cpu())\n",
    "        output[batch_idx*batch_size:(batch_idx+1)*batch_size] = out\n",
    "        print(f\"Batch number: {batch_idx+1}/{len(complete_dataloader)}\", end=\"\\r\", flush=True)\n",
    "\n",
    "\n",
    "output = torch.softmax(output, dim=1).argmax(dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "1    1258648\n",
      "0    1114188\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>body</th>\n",
       "      <th>top_wick</th>\n",
       "      <th>bottom_wick</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02 18:01:00-05:00</th>\n",
       "      <td>4888.00</td>\n",
       "      <td>4888.50</td>\n",
       "      <td>4887.00</td>\n",
       "      <td>4887.00</td>\n",
       "      <td>90.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 18:02:00-05:00</th>\n",
       "      <td>4887.25</td>\n",
       "      <td>4888.00</td>\n",
       "      <td>4886.75</td>\n",
       "      <td>4887.75</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 18:03:00-05:00</th>\n",
       "      <td>4887.75</td>\n",
       "      <td>4888.00</td>\n",
       "      <td>4887.50</td>\n",
       "      <td>4888.00</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 18:05:00-05:00</th>\n",
       "      <td>4889.75</td>\n",
       "      <td>4890.00</td>\n",
       "      <td>4887.50</td>\n",
       "      <td>4888.00</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02 18:07:00-05:00</th>\n",
       "      <td>4887.50</td>\n",
       "      <td>4888.00</td>\n",
       "      <td>4887.00</td>\n",
       "      <td>4887.75</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26 14:37:00-04:00</th>\n",
       "      <td>14282.75</td>\n",
       "      <td>14284.75</td>\n",
       "      <td>14271.00</td>\n",
       "      <td>14272.50</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26 14:39:00-04:00</th>\n",
       "      <td>14268.75</td>\n",
       "      <td>14269.25</td>\n",
       "      <td>14264.00</td>\n",
       "      <td>14265.75</td>\n",
       "      <td>797.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26 14:40:00-04:00</th>\n",
       "      <td>14268.25</td>\n",
       "      <td>14268.25</td>\n",
       "      <td>14256.75</td>\n",
       "      <td>14259.50</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26 14:43:00-04:00</th>\n",
       "      <td>14281.25</td>\n",
       "      <td>14281.25</td>\n",
       "      <td>14281.00</td>\n",
       "      <td>14281.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26 14:44:00-04:00</th>\n",
       "      <td>14283.25</td>\n",
       "      <td>14284.00</td>\n",
       "      <td>14276.75</td>\n",
       "      <td>14278.75</td>\n",
       "      <td>541.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258648 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               open      high       low     close  volume  \\\n",
       "date                                                                        \n",
       "2017-01-02 18:01:00-05:00   4888.00   4888.50   4887.00   4887.00    90.0   \n",
       "2017-01-02 18:02:00-05:00   4887.25   4888.00   4886.75   4887.75    70.0   \n",
       "2017-01-02 18:03:00-05:00   4887.75   4888.00   4887.50   4888.00    40.0   \n",
       "2017-01-02 18:05:00-05:00   4889.75   4890.00   4887.50   4888.00   116.0   \n",
       "2017-01-02 18:07:00-05:00   4887.50   4888.00   4887.00   4887.75    53.0   \n",
       "...                             ...       ...       ...       ...     ...   \n",
       "2023-10-26 14:37:00-04:00  14282.75  14284.75  14271.00  14272.50  1981.0   \n",
       "2023-10-26 14:39:00-04:00  14268.75  14269.25  14264.00  14265.75   797.0   \n",
       "2023-10-26 14:40:00-04:00  14268.25  14268.25  14256.75  14259.50  1044.0   \n",
       "2023-10-26 14:43:00-04:00  14281.25  14281.25  14281.00  14281.00     5.0   \n",
       "2023-10-26 14:44:00-04:00  14283.25  14284.00  14276.75  14278.75   541.0   \n",
       "\n",
       "                           body  top_wick  bottom_wick  class  \n",
       "date                                                           \n",
       "2017-01-02 18:01:00-05:00  -4.0       2.0          0.0      1  \n",
       "2017-01-02 18:02:00-05:00   2.0       1.0          2.0      1  \n",
       "2017-01-02 18:03:00-05:00   1.0       0.0          1.0      1  \n",
       "2017-01-02 18:05:00-05:00  -7.0       1.0          2.0      1  \n",
       "2017-01-02 18:07:00-05:00   1.0       1.0          2.0      1  \n",
       "...                         ...       ...          ...    ...  \n",
       "2023-10-26 14:37:00-04:00 -41.0       8.0          6.0      1  \n",
       "2023-10-26 14:39:00-04:00 -12.0       2.0          7.0      1  \n",
       "2023-10-26 14:40:00-04:00 -35.0       0.0         11.0      1  \n",
       "2023-10-26 14:43:00-04:00  -1.0       0.0          0.0      1  \n",
       "2023-10-26 14:44:00-04:00 -18.0       3.0          8.0      1  \n",
       "\n",
       "[1258648 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df['class'].value_counts())\n",
    "df[df['class'] == 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
