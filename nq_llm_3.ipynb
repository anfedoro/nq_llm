{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2372830, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#work with body vocab\n",
    "\n",
    "#load data from hdf5 file and create a dataset\n",
    "with h5py.File(\"data/nq17-23_1min_candle_seq_1024.hdf5\", \"r\") as f:\n",
    "    # total_records = f[\"data\"].shape[0]\n",
    "    # start_index = int(total_records*0.55)\n",
    "    # stop_index = int(total_records*0.65)\n",
    "    dataset = f[\"data\"][:]\n",
    "    dataset = torch.from_numpy(dataset).long()\n",
    "#load index to candlestick mapping from hdf5 file\n",
    "\n",
    "    index_to_candle = {}\n",
    "    for key in f.keys():\n",
    "        if key != \"data\":\n",
    "            group = f[key]\n",
    "            sizes = group['sizes'][:]\n",
    "            direction = group['direction'][()]\n",
    "            index_to_candle[int(key)] = {'sizes':tuple(sizes), 'direction':direction}\n",
    "    \n",
    "\n",
    "candle_seq_len = 6\n",
    "dataset = dataset[torch.where(dataset != 1024)]\n",
    "dataset = dataset.unfold(0, candle_seq_len, 1)\n",
    "input_dataset = dataset[:, :-1]\n",
    "target_dataset = dataset[: , 1:]\n",
    "\n",
    "dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([514, 883, 852, 248, 591]), tensor([883, 852, 248, 591, 615]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0], target_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "split_idx = int(len(dataset)*0.8)\n",
    "train_data = input_dataset[:split_idx]\n",
    "train_targets = target_dataset[:split_idx]\n",
    "test_data = input_dataset[split_idx:]\n",
    "test_targets = target_dataset[split_idx:]\n",
    "\n",
    "val_split_idx = int(len(train_data)*0.8)\n",
    "val_data = train_data[val_split_idx:]\n",
    "val_targets = train_targets[val_split_idx:]\n",
    "train_data = train_data[:val_split_idx]\n",
    "train_targets = train_targets[:val_split_idx]\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_targets)\n",
    "val_dataset = TensorDataset(val_data, val_targets)\n",
    "test_dataset = TensorDataset(test_data, test_targets)\n",
    "\n",
    "batch_size = 32\n",
    "#load data into dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function to restore candles from their codes and plot candlestick chart to writer\n",
    "import plotly.graph_objects as go\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def full_candle_restore(candle_index:(np.array,torch.Tensor), start_price:float = 1000, number_of_candles = None, index_map = index_to_candle) -> pd.DataFrame:\n",
    "        \n",
    "        '''\n",
    "        Restore full candle from index of candle and start price\n",
    "        '''\n",
    "        tick_size = 0.25\n",
    "        candle_index = candle_index[np.where(candle_index != 1024)]\n",
    "\n",
    "        if isinstance(candle_index, torch.Tensor):\n",
    "            candle_index = candle_index.numpy()\n",
    "        if number_of_candles is not None:\n",
    "            candle_index = candle_index[:number_of_candles]\n",
    "        \n",
    "        candles = []\n",
    "        for idx, cdl_idx in enumerate(candle_index):\n",
    "            top_wick, body, bottom_wick = index_map[cdl_idx]['sizes']\n",
    "            direction = index_map[cdl_idx]['direction']\n",
    "            candle = {}\n",
    "            if idx == 0:\n",
    "                candle['open'] = start_price\n",
    "            else:\n",
    "                candle['open'] = candles[-1]['close']\n",
    "            \n",
    "            close = candle['open'] + body * tick_size * direction\n",
    "            high = close + top_wick * tick_size if close > candle['open'] else candle['open'] + top_wick * tick_size\n",
    "            low = candle['open'] - bottom_wick * tick_size if close > candle['open'] else close - bottom_wick * tick_size\n",
    "            candle['high'] = high\n",
    "            candle['low'] = low\n",
    "            candle['close'] = close\n",
    "            candles.append(candle)\n",
    "    \n",
    "        return pd.DataFrame(candles)\n",
    "\n",
    "def write_charts_to_TB(name,writer, targets, outputs, epoch):\n",
    "    \n",
    "    \n",
    "    #convert outputs logit to candlestick codes\n",
    "    outputs = outputs.softmax(dim=1).argmax(dim=1)\n",
    "\n",
    "    \n",
    "\n",
    "    original_candles = full_candle_restore(targets)\n",
    "    predicted_candles = full_candle_restore(outputs)\n",
    "    fig1 = go.Figure(data=[go.Candlestick(x=original_candles.index,\n",
    "                open=original_candles['open'],\n",
    "                high=original_candles['high'],\n",
    "                low=original_candles['low'],\n",
    "                close=original_candles['close'])])\n",
    "    #increase chart size\n",
    "    fig1.update_layout(height=600, width=1200)\n",
    "    fig1.update_layout(xaxis_rangeslider_visible=False)\n",
    "    \n",
    "\n",
    "    fig2 = go.Figure(data=[go.Candlestick(x=predicted_candles.index,\n",
    "                    open=predicted_candles['open'],\n",
    "                    high=predicted_candles['high'],\n",
    "                    low=predicted_candles['low'],\n",
    "                    close=predicted_candles['close'])])\n",
    "    fig2.update_layout(height=600, width=1200)\n",
    "    fig2.update_layout(xaxis_rangeslider_visible=False)\n",
    "    \n",
    "    #convert figures to image and write to tensorboard\n",
    "    fig1_bytes = fig1.to_image(format=\"png\")\n",
    "    fig2_bytes = fig2.to_image(format=\"png\")\n",
    "\n",
    "    # Преобразуем байтовые данные в массив NumPy\n",
    "    fig1_image = np.array(Image.open(io.BytesIO(fig1_bytes)))\n",
    "    fig2_image = np.array(Image.open(io.BytesIO(fig2_bytes)))\n",
    "\n",
    "    # Преобразуем массивы NumPy в тензоры PyTorch\n",
    "  \n",
    "\n",
    "    # Добавляем изображения в TensorBoard\n",
    "    writer.add_image(f'{name}_original', fig1_image, epoch, dataformats='HWC')\n",
    "    writer.add_image(f'{name}_predicted', fig2_image, epoch, dataformats='HWC')\n",
    "\n",
    "def write_charts_to_sceeen(name, writer, targets, outputs, epoch):\n",
    "    \n",
    " \n",
    "    #convert outputs logit to candlestick codes\n",
    "    outputs = outputs.softmax(dim=1).argmax(dim=1)\n",
    "\n",
    "\n",
    "    original_candles = full_candle_restore(targets)\n",
    "    predicted_candles = full_candle_restore(outputs)\n",
    "    fig1 = go.Figure(data=[go.Candlestick(x=original_candles.index,\n",
    "                open=original_candles['open'],\n",
    "                high=original_candles['high'],\n",
    "                low=original_candles['low'],\n",
    "                close=original_candles['close'])])\n",
    "    #increase chart size\n",
    "    fig1.update_layout(height=600, width=1200)\n",
    "    fig1.update_layout(xaxis_rangeslider_visible=False)\n",
    "    \n",
    "\n",
    "    fig2 = go.Figure(data=[go.Candlestick(x=predicted_candles.index,\n",
    "                    open=predicted_candles['open'],\n",
    "                    high=predicted_candles['high'],\n",
    "                    low=predicted_candles['low'],\n",
    "                    close=predicted_candles['close'])])\n",
    "    fig2.update_layout(height=600, width=1200)\n",
    "    fig2.update_layout(xaxis_rangeslider_visible=False)\n",
    "    \n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)  # Изменение размеров для соответствия формату batch_first\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)  \n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "        # print('Enbedding shape:', src.shape)\n",
    "        src = self.pos_encoder(src)\n",
    "        # print('Positional encoding shape:', src.shape)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # print('Transformer encoder output shape:', output.shape)\n",
    "    \n",
    "        output = self.output_layer(output)\n",
    "        # print('Output layer shape:', output.shape)\n",
    "        return output #return only last output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of model parameters\n",
    "vocab_size = dataset.max() + 1\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "#check if we have CUDA or MPS and setup respecive device, if not CUDA nor MPS is available, then use CPU\n",
    "def init_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "    #device = torch.device('cpu')\n",
    "\n",
    "    print(f\"Device to be used: {device}\")\n",
    "    #Initialize model\n",
    "    torch.manual_seed(42)\n",
    "    model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers, dropout)\n",
    "\n",
    "    model = model.to(device)\n",
    "    #print(model)\n",
    "    #print model device\n",
    "    next(model.parameters()).device\n",
    "    return model, device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test model forward pass\n",
    "# for idx, (data, target) in enumerate(train_loader):\n",
    "    # model, device = init_model()\n",
    "    # data = data.to(device)\n",
    "    # target = target.to(device)\n",
    "    # output = model(data)\n",
    "    # print(f'Output shape: {output.shape}')\n",
    "    # print(f'Input data shape: {data.shape}')\n",
    "    # print(f'Target shape: {target.shape}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define writer for tensorboard\n",
    "import os\n",
    "delete_logs = True\n",
    "if delete_logs:\n",
    "    #use python os package to delete logs including files in subfolders and subfolders itself\n",
    "    for root, dirs, files in os.walk('./runs/nq_llm_2_5min'):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            for fils in os.listdir(os.path.join(root, dir)):\n",
    "                os.remove(os.path.join(root, dir, fils))\n",
    "            os.rmdir(os.path.join(root, dir))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "writer = SummaryWriter('runs/nq_llm_2_5min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used: mps\n"
     ]
    }
   ],
   "source": [
    "#initialize model, loss function and optimizer\n",
    "model, device = init_model()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001 )  #, weight_decay=1e-5)\n",
    "seq_len = candle_seq_len - 1\n",
    "\n",
    "loss_weights = torch.ones(seq_len).to(device)\n",
    "loss_weights[-1] = 5.0\n",
    "\n",
    "best_vloss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #caclulate number of parameters in the model\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0.001\n",
      "41\n",
      "5.690683002568377\n"
     ]
    }
   ],
   "source": [
    "#restore model  and optimizer state  rom checkpoint './models/nq-llm_0_2.pth'\n",
    "\n",
    "checkpoint = torch.load('./models/nq-llm_0_2_5min.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_vloss = checkpoint['vloss']\n",
    "#print optimizer state\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group['lr'])\n",
    "    print(param_group['weight_decay'])\n",
    "print(epoch)\n",
    "print(best_vloss)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct adjust optimizer learning rate and weight decay\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 1e-4\n",
    "    g['weight_decay'] = 1e-3\n",
    "\n",
    "loss_weights[-1] = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with validation loss 5.682681 Learning rate: 1.00e-04 Weight decay: 1.00e-03 \n",
      "Epoch [1/100], Train Loss: 5.425280, Validation Loss: 5.682681, Validation accuracy: 1.09% \n",
      "Model saved at epoch 2 with validation loss 5.677697 Learning rate: 1.00e-04 Weight decay: 1.00e-03 \n",
      "Epoch [2/100], Train Loss: 5.410542, Validation Loss: 5.677697, Validation accuracy: 1.07% \n",
      "Epoch [3/100], Train Loss: 5.401960, Validation Loss: 5.679494, Validation accuracy: 1.04% \n",
      "Epoch [4/100], Train Loss: 5.394966, Validation Loss: 5.680945, Validation accuracy: 1.09% \n",
      "Model saved at epoch 5 with validation loss 5.674621 Learning rate: 1.00e-04 Weight decay: 1.00e-03 \n",
      "Epoch [5/100], Train Loss: 5.388639, Validation Loss: 5.674621, Validation accuracy: 1.09% \n",
      "Epoch [6/100], Train Loss: 5.383979, Validation Loss: 5.677809, Validation accuracy: 1.08% \n",
      "Epoch [7/100], Train Loss: 5.379053, Validation Loss: 5.681036, Validation accuracy: 1.08% \n",
      "Epoch [8/100], Train Loss: 5.374539, Validation Loss: 5.682764, Validation accuracy: 1.09% \n",
      "Epoch [9/100], Train Loss: 5.371058, Validation Loss: 5.682950, Validation accuracy: 1.09% \n",
      "Epoch [10/100], Train Loss: 5.366531, Validation Loss: 5.687140, Validation accuracy: 1.05% \n",
      "Epoch [11/100], Train Loss: 5.363285, Validation Loss: 5.685483, Validation accuracy: 1.09% \n",
      "Epoch [12/100], Train Loss: 5.359327, Validation Loss: 5.692006, Validation accuracy: 1.08% \n",
      "Epoch [13/100], Train Loss: 5.356031, Validation Loss: 5.698875, Validation accuracy: 1.05% \n",
      "Epoch [14/100], Train Loss: 5.352772, Validation Loss: 5.700462, Validation accuracy: 1.01% \n",
      "Interrupted at epoch 14 with Training loss 29,554.490128 and Validation loss 67,635.979230\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "try:\n",
    "    for epoch in range(start_epoch,num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "\n",
    "            #loss caclulation with weighted last candle\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "            loss = loss.view(-1, seq_len)\n",
    "            weighted_loss = loss * loss_weights\n",
    "            loss = weighted_loss.mean()\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():,.6f}', end='\\r', flush=True)\n",
    "      \n",
    "        \n",
    "            \n",
    "\n",
    "        #add weights and biases to tensorboard\n",
    "        weights = {}\n",
    "        biases = {}\n",
    "        grads = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                writer.add_histogram(f'weights/{name}', param, epoch)\n",
    "            elif 'bias' in name:\n",
    "                writer.add_histogram(f'biases/{name}', param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'grads/{name}', param.grad, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        # Test the model\n",
    "        \n",
    "        model.eval()\n",
    "        vepoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for vbatch_idx, (vdata, vlabels) in enumerate(val_loader):\n",
    "                vdata = vdata.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vdata)\n",
    "                #loss caclulation with weighted last candle\n",
    "                vloss = loss_fn(voutputs.view(-1, vocab_size), vlabels.view(-1))\n",
    "                vloss = vloss.view(-1, seq_len)\n",
    "                vweighted_loss = vloss * loss_weights\n",
    "                vloss = vweighted_loss.mean()\n",
    "                vepoch_loss += vloss.item()\n",
    "\n",
    "                #calculate accuracy\n",
    "                last_redicted_candle = voutputs[:,-1,:].softmax(dim=1).argmax(dim=1)\n",
    "                last_actual_candle = vlabels[:,-1]\n",
    "                correct += (last_redicted_candle == last_actual_candle).sum().item()\n",
    "                total += last_actual_candle.size(0)\n",
    "                accuracy = correct / total * 100\n",
    "                \n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{vbatch_idx + 1}/{len(val_loader)}], Validation Loss: {vloss.item():,.6f}, Validation accuracy: {accuracy:.2f}%', end='\\r', flush=True)\n",
    "    \n",
    "\n",
    "        # Save the model checkpoint if validation loss is less than best validation loss\n",
    "        if vepoch_loss/len(val_loader) < best_vloss:\n",
    "            best_vloss = vepoch_loss/len(val_loader)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'vocab_size': vocab_size,\n",
    "                'embed_dim': embed_dim,\n",
    "                'num_heads': num_heads,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout': dropout,\n",
    "                'vloss': best_vloss,\n",
    "            \n",
    "                }, './models/nq-llm_0_2_5min.pth')\n",
    "            lr = next(iter(optimizer.param_groups))['lr']\n",
    "            weight_decay = next(iter(optimizer.param_groups))['weight_decay']\n",
    "            print(f\"Model saved at epoch {epoch+1} with validation loss {vepoch_loss/len(val_loader):.6f} Learning rate: {lr:.2e} Weight decay: {weight_decay:.2e} \")\n",
    "        #else - restore the model from previous checkpoint and reduce learning rate 5 times and increase weight decay 50%\n",
    "        # else:\n",
    "        #     checkpoint = torch.load('./models/nq-lstm.pth')\n",
    "        #     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "        #     for g in optimizer.param_groups:\n",
    "        #         g['lr'] = g['lr'] * 0.3\n",
    "        #         g['weight_decay'] = g['weight_decay'] * 1.1\n",
    "        #     print(f\"Model restored from epoch {epoch} with validation loss {best_vloss/len(val_loader)}\")\n",
    "        #     print(f\"Learning rate reduced to {g['lr']} and weight decay increased to {g['weight_decay']}\")\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss / len(train_loader):,.6f}, Validation Loss: {vepoch_loss / len(val_loader):,.6f}, Validation accuracy: {accuracy:.2f}% ')\n",
    "        writer.add_scalars('Loss', {'Train': epoch_loss / len(train_loader), 'Test': vepoch_loss / len(val_loader)}, epoch)\n",
    "        writer.add_scalar('Accuracy', accuracy, epoch)\n",
    "        \n",
    "        \n",
    "        # get actual lr and weight decay from optimizer and write them to tensorboard\n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "        weight_decay = next(iter(optimizer.param_groups))['weight_decay']\n",
    "        writer.add_scalar('Learning rate', lr, epoch)\n",
    "        writer.add_scalar('Weight decay', weight_decay, epoch)\n",
    "        \n",
    "        #write charts to tensorboard\n",
    "        write_charts_to_TB('Test data sample',writer, vlabels[0].cpu(), voutputs[0].cpu(), epoch)\n",
    "        write_charts_to_TB('Test predicted candles sequence',writer, vlabels[:60,-1].cpu(), voutputs[:60,-1,:].cpu(), epoch)\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Interrupted at epoch {epoch} with Training loss {epoch_loss:,.6f} and Validation loss {vepoch_loss:,.6f}\")\n",
    "finally:\n",
    "    writer.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function which update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_charts_to_TB('Test data sample',writer, vlabels[0].cpu(), voutputs[0].cpu(), 100, index_to_candle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interrim model parameters and optimizer state saving\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'batch_step': batch_idx,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'vloss': vepoch_loss,\n",
    "            }, './models/nq-llm_0_2_interrim.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which perform candle prediction in the following way:\n",
    "# 1. take last 30 tockenized candles from test dataset\n",
    "# 2. predict next tocken\n",
    "# 3. add predicted tocken to the end of tockenized candles and remove first tocken\n",
    "# 4. repeat 2-3 steps until new candle tocken (0) is predicted.\n",
    "# 5. return predicted tockenized candle along with preceding 30 tockenized candles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, (vdata, vlabels) in enumerate(test_loader):\n",
    "        vdata = vdata.to(device)\n",
    "        vlabels = vlabels.to(device)\n",
    "        voutputs = model(vdata)    \n",
    "        break \n",
    "F.softmax(voutputs,dim=1).argmax(dim=1), vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#test the function\n",
    "write_charts_to_TB('Test data sample',writer, vlabels.cpu(), voutputs.cpu(), 200, index_to_body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
