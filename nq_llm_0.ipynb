{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sequencedataset import CandleSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from hdf5 file and create a dataset\n",
    "with h5py.File(\"data/nq17-23_1min_seq.hdf5\", \"r\") as f:\n",
    "    total_records = f[\"data\"].shape[0]\n",
    "    # start_index = int(total_records*0.75)\n",
    "    # stop_index = int(total_records*0.80)\n",
    "    dataset = f[\"data\"][:] #[start_index:stop_index]\n",
    "\n",
    "\n",
    "split_idx = int(len(dataset)*0.8)\n",
    "train_data = dataset[:split_idx]\n",
    "test_data = dataset[split_idx:]\n",
    "\n",
    "train_dataset = CandleSeriesDataset(train_data)\n",
    "test_dataset = CandleSeriesDataset(test_data)\n",
    "\n",
    "batch_size = 256\n",
    "#load data into dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)  # Изменение размеров для соответствия формату batch_first\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)  \n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * math.sqrt(self.embed_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.output_layer(output)\n",
    "        return output[:, -1, :] #return only last output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of model parameters\n",
    "vocab_size = dataset.max()\n",
    "embed_dim = 32\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "#check if we have CUDA or MPS and setup respecive device, if not CUDA nor MPS is available, then use CPU\n",
    "def init_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "    #device = torch.device('cpu')\n",
    "\n",
    "    print(f\"Device to be used: {device}\")\n",
    "    #Initialize model\n",
    "    torch.manual_seed(42)\n",
    "    model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers, dropout)\n",
    "\n",
    "    model = model.to(device)\n",
    "    #print(model)\n",
    "    #print model device\n",
    "    next(model.parameters()).device\n",
    "    return model, device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test model forward pass\n",
    "# for idx, (data, target) in enumerate(train_loader):\n",
    "#     model, device = init_model()\n",
    "#     data = data.to(device)\n",
    "#     target = target.to(device)\n",
    "#     output = model(data)\n",
    "#     print(output.shape)\n",
    "#     print(data.shape)\n",
    "#     print(target.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define writer for tensorboard\n",
    "import os\n",
    "delete_logs = True\n",
    "if delete_logs:\n",
    "    #use python os package to delete logs including files in subfolders and subfolders itself\n",
    "    for root, dirs, files in os.walk('./runs/nq_llm_0'):\n",
    "        for file in files:\n",
    "            os.remove(os.path.join(root, file))\n",
    "        for dir in dirs:\n",
    "            for fils in os.listdir(os.path.join(root, dir)):\n",
    "                os.remove(os.path.join(root, dir, fils))\n",
    "            os.rmdir(os.path.join(root, dir))   \n",
    "    \n",
    "writer = SummaryWriter('runs/nq_llm_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device to be used: mps\n"
     ]
    }
   ],
   "source": [
    "#initialize model, loss function and optimizer\n",
    "model, device = init_model()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "best_vloss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349563"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #caclulate number of parameters in the model\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "#restore model  and optimizer state from checkpoint './models/nq-llm_0.pth'\n",
    "\n",
    "checkpoint = torch.load('./models/nq-llm_0.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "best_vloss = checkpoint['vloss']\n",
    "#print optimizer state\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group['lr'])\n",
    "    print(param_group['weight_decay'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct adjust optimizer learning rate and weight decay\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 1e-4\n",
    "    g['weight_decay'] = 1e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with validation loss 0.005562 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [1/100], Train Loss: 4.021925, Validation Loss: 5.450962\n",
      "Model saved at epoch 2 with validation loss 0.005556 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [2/100], Train Loss: 4.072854, Validation Loss: 5.445582\n",
      "Model saved at epoch 3 with validation loss 0.005546 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [3/100], Train Loss: 4.029094, Validation Loss: 5.435732\n",
      "Model saved at epoch 4 with validation loss 0.005545 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [4/100], Train Loss: 4.048240, Validation Loss: 5.434364\n",
      "Model saved at epoch 5 with validation loss 0.005534 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [5/100], Train Loss: 4.081314, Validation Loss: 5.423396\n",
      "Model saved at epoch 6 with validation loss 0.005528 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [6/100], Train Loss: 4.000529, Validation Loss: 5.417753\n",
      "Epoch [7/100], Train Loss: 4.045879, Validation Loss: 5.421694\n",
      "Epoch [8/100], Train Loss: 4.024223, Validation Loss: 5.426307\n",
      "Epoch [9/100], Train Loss: 4.072255, Validation Loss: 5.419487\n",
      "Epoch [10/100], Train Loss: 4.044713, Validation Loss: 5.419365\n",
      "Epoch [11/100], Train Loss: 4.052655, Validation Loss: 5.421072\n",
      "Epoch [12/100], Train Loss: 4.077356, Validation Loss: 5.425596\n",
      "Epoch [13/100], Train Loss: 4.001923, Validation Loss: 5.434763\n",
      "Epoch [14/100], Train Loss: 4.073383, Validation Loss: 5.435988\n",
      "Epoch [15/100], Train Loss: 4.011880, Validation Loss: 5.425466\n",
      "Epoch [16/100], Train Loss: 4.013478, Validation Loss: 5.434255\n",
      "Epoch [17/100], Train Loss: 4.063873, Validation Loss: 5.435134\n",
      "Epoch [18/100], Train Loss: 4.035915, Validation Loss: 5.421236\n",
      "Epoch [19/100], Train Loss: 4.044919, Validation Loss: 5.422002\n",
      "Epoch [20/100], Train Loss: 4.044979, Validation Loss: 5.419885\n",
      "Model saved at epoch 21 with validation loss 0.005522 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [21/100], Train Loss: 4.076111, Validation Loss: 5.412322\n",
      "Model saved at epoch 22 with validation loss 0.005522 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [22/100], Train Loss: 4.078590, Validation Loss: 5.412128\n",
      "Epoch [23/100], Train Loss: 4.038406, Validation Loss: 5.416859\n",
      "Epoch [24/100], Train Loss: 3.987387, Validation Loss: 5.422912\n",
      "Epoch [25/100], Train Loss: 4.022208, Validation Loss: 5.428187\n",
      "Epoch [26/100], Train Loss: 4.020096, Validation Loss: 5.419045\n",
      "Epoch [27/100], Train Loss: 4.037519, Validation Loss: 5.414783\n",
      "Model saved at epoch 28 with validation loss 0.005521 Learning rate: 1.00e-04 Weight decay: 1.00e-05 \n",
      "Epoch [28/100], Train Loss: 4.068361, Validation Loss: 5.411333\n",
      "Epoch [29/100], Train Loss: 3.985701, Validation Loss: 5.420626\n",
      "Epoch [30/100], Train Loss: 3.999927, Validation Loss: 5.434526\n",
      "Epoch [31/100], Train Loss: 4.052537, Validation Loss: 5.441893\n",
      "Epoch [32/100], Train Loss: 4.051893, Validation Loss: 5.433692\n",
      "Epoch [33/100], Train Loss: 4.042672, Validation Loss: 5.430288\n",
      "Epoch [34/100], Train Loss: 4.007357, Validation Loss: 5.439539\n",
      "Epoch [35/100], Train Loss: 4.009709, Validation Loss: 5.437108\n",
      "Epoch [36/100], Train Loss: 4.002975, Validation Loss: 5.433380\n",
      "Interrupted at epoch 36 with Training loss 32.900955 and Validation loss 54.333797\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "num_epochs = start_epoch + 100\n",
    "bath_limit = 10\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "try:\n",
    "    for epoch in range(start_epoch,num_epochs):\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():,.6f}', end='\\r', flush=True)\n",
    "            if batch_idx >= bath_limit:\n",
    "                break\n",
    "        \n",
    "            \n",
    "\n",
    "        #add weights and biases to tensorboard\n",
    "        weights = {}\n",
    "        biases = {}\n",
    "        grads = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                writer.add_histogram(f'weights/{name}', param, epoch)\n",
    "            elif 'bias' in name:\n",
    "                writer.add_histogram(f'biases/{name}', param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'grads/{name}', param.grad, epoch)\n",
    "\n",
    "\n",
    "\n",
    "        # Test the model\n",
    "        \n",
    "        model.eval()\n",
    "        vepoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for vbatch_idx, (vdata, vlabels) in enumerate(test_loader):\n",
    "                vdata = vdata.to(device)\n",
    "                vlabels = vlabels.to(device)\n",
    "                voutputs = model(vdata)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                vepoch_loss += vloss.item()\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{vbatch_idx + 1}/{len(test_loader)}], Validation Loss: {vloss.item():,.6f}', end='\\r', flush=True)\n",
    "                if vbatch_idx >= bath_limit:\n",
    "                    break\n",
    "\n",
    "        # Save the model checkpoint if validation loss is less than best validation loss\n",
    "        if vepoch_loss/len(test_loader) < best_vloss:\n",
    "            best_vloss = vepoch_loss/len(test_loader)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'vloss': best_vloss,\n",
    "                }, './models/nq-llm_0_1.pth')\n",
    "            lr = next(iter(optimizer.param_groups))['lr']\n",
    "            weight_decay = next(iter(optimizer.param_groups))['weight_decay']\n",
    "            print(f\"Model saved at epoch {epoch+1} with validation loss {vepoch_loss/len(test_loader):.6f} Learning rate: {lr:.2e} Weight decay: {weight_decay:.2e} \")\n",
    "        #else - restore the model from previous checkpoint and reduce learning rate 5 times and increase weight decay 50%\n",
    "        # else:\n",
    "        #     checkpoint = torch.load('./models/nq-lstm.pth')\n",
    "        #     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "        #     for g in optimizer.param_groups:\n",
    "        #         g['lr'] = g['lr'] * 0.3\n",
    "        #         g['weight_decay'] = g['weight_decay'] * 1.1\n",
    "        #     print(f\"Model restored from epoch {epoch} with validation loss {best_vloss/len(test_loader)}\")\n",
    "        #     print(f\"Learning rate reduced to {g['lr']} and weight decay increased to {g['weight_decay']}\")\n",
    "\n",
    "        \n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss / batch_idx:,.6f}, Validation Loss: {vepoch_loss / vbatch_idx:,.6f}')\n",
    "        writer.add_scalars('Loss', {'Train': epoch_loss / batch_idx, 'Test': vepoch_loss / vbatch_idx}, epoch)\n",
    "        \n",
    "        \n",
    "        # get actual lr and weight decay from optimizer and write them to tensorboard\n",
    "        lr = next(iter(optimizer.param_groups))['lr']\n",
    "        weight_decay = next(iter(optimizer.param_groups))['weight_decay']\n",
    "        writer.add_scalar('Learning rate', lr, epoch)\n",
    "        writer.add_scalar('Weight decay', weight_decay, epoch)\n",
    "        \n",
    "        # write_charts_to_TB('Test data 1m candles',writer, vlabels.cpu().numpy()[:31,-1,0:4].reshape(-1,4), voutputs.cpu().numpy()[:31,-1,0:4].reshape(-1,4), epoch)\n",
    "        # write_charts_to_TB('Test data sample',writer, vlabels.cpu().numpy()[30,-31:,0:4].reshape(-1,4), voutputs.cpu().numpy()[30,-31:,0:4].reshape(-1,4), epoch)\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Interrupted at epoch {epoch} with Training loss {epoch_loss:,.6f} and Validation loss {vepoch_loss:,.6f}\")\n",
    "finally:\n",
    "    writer.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interrim model parameters and optimizer state saving\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'batch_step': batch_idx,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "            }, './models/nq-llm_0_interrim.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which perform candle prediction in the following way:\n",
    "# 1. take last 30 tockenized candles from test dataset\n",
    "# 2. predict next tocken\n",
    "# 3. add predicted tocken to the end of tockenized candles and remove first tocken\n",
    "# 4. repeat 2-3 steps until new candle tocken (0) is predicted.\n",
    "# 5. return predicted tockenized candle along with preceding 30 tockenized candles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  9, 212,   9, 975,   0,   9, 209,   9, 975,   0,   9, 212, 587, 975,\n",
       "           0,   9, 212, 587, 975,   0,   9, 212, 587, 975,   0,   1,   9, 212,\n",
       "         587, 975,   0,   9, 209,   9, 975,   0,   9, 209,   9, 975,   0,   9,\n",
       "         212,   9, 975,   0,   9, 209,   9, 975,   0,   1,   2,   9, 212,   9,\n",
       "         975,   0,   9, 215,  13, 975,   0,   9], device='mps:0'),\n",
       " tensor([587, 225,   9, 974,   0, 589, 232,  17, 975,   0,  11, 229, 596, 976,\n",
       "           0,  10, 227, 587, 976,   0,   9, 222, 597, 975,   0,   1,  10, 209,\n",
       "         599, 974,   0, 588, 229,  14, 978,   0, 594, 209,  14, 975,   0, 590,\n",
       "         210,  10, 974,   0, 588, 221,  16, 979,   0,   1,   2, 587, 238,  12,\n",
       "         978,   0, 587, 220,  39, 976,   0, 591], device='mps:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, (vdata, vlabels) in enumerate(test_loader):\n",
    "        vdata = vdata.to(device)\n",
    "        vlabels = vlabels.to(device)\n",
    "        voutputs = model(vdata)    \n",
    "        break \n",
    "F.softmax(voutputs,dim=1).argmax(dim=1), vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save sequense tensor to hdf5\n",
    "# with h5py.File('data/nq17-23_1min_seq.hdf5', 'w') as f:\n",
    "#        dataset = f.create_dataset('data', shape=seq_tensor.shape, dtype='i8')\n",
    "#        dataset[:] = seq_tensor[:]\n",
    "#        #save index_to_value dictionaries\n",
    "#        f.create_dataset('index_to_bottom_wick', data=np.array(list(index_to_bottom_wick.items())))\n",
    "#        f.create_dataset('index_to_body', data=np.array(list(index_to_body.items())))\n",
    "#        f.create_dataset('index_to_top_wick', data=np.array(list(index_to_top_wick.items())))\n",
    "#        f.create_dataset('index_to_open_gap', data=np.array(list(index_to_open_gap.items())))\n",
    "\n",
    "#load index to candlestick mapping from hdf5 file\n",
    "with h5py.File(\"data/nq17-23_1min_seq.hdf5\", \"r\") as f:\n",
    "    index_to_bottom_wick = dict(f[\"index_to_bottom_wick\"][:])\n",
    "    index_to_body = dict(f[\"index_to_body\"][:])\n",
    "    index_to_top_wick = dict(f[\"index_to_top_wick\"][:])\n",
    "    index_to_open_gap = dict(f[\"index_to_open_gap\"][:])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
